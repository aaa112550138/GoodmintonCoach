import os
import io
import platform
import pandas as pd
from dotenv import load_dotenv
import matplotlib.font_manager as fm
import traceback # --- [æ–°å¢]ï¼šç”¨æ–¼å°å‡ºè©³ç´°éŒ¯èª¤ ---

# --- é—œéµï¼šå¾ä½ çš„ Streamlit å°ˆæ¡ˆä¸­ï¼ŒæŠŠé€™äº›æª”æ¡ˆ/è³‡æ–™å¤¾è¤‡è£½éä¾† ---
try:
    from utils.data_loader import load_all_data
    from config.prompts import create_system_prompt
except ImportError:
    print("="*50)
    print("éŒ¯èª¤ï¼šè«‹ç¢ºä¿ 'utils' å’Œ 'config' è³‡æ–™å¤¾å­˜åœ¨ã€‚")
    print("="*50)
    raise

# --- é—œéµï¼šæˆ‘å€‘ã€Œç›´æ¥ã€åŒ¯å…¥ Google å®˜æ–¹å¥—ä»¶ ---
try:
    import google.generativeai as genai
except ImportError:
    print("="*50)
    print("éŒ¯èª¤ï¼šæ‰¾ä¸åˆ° 'google-generativeai' å¥—ä»¶ã€‚")
    print("è«‹åŸ·è¡Œï¼š pip install google-generativeai")
    print("="*50)
    raise

# --- åˆå§‹è¨­å®š ---
print("[llm_core DEBUG] æ­£åœ¨è¼‰å…¥ .env æª”æ¡ˆ...")
load_dotenv()

# --- 1. è¼‰å…¥è³‡æ–™ (ä¿æŒä¸è®Š) ---
df, data_schema_info, column_definitions_info = load_all_data()
if df is None:
    print("="*50)
    print("è­¦å‘Š [llm_core]: 'all_dataset.csv' æª”æ¡ˆè¼‰å…¥å¤±æ•—ã€‚")
    print("="*50)

# --- 2. [å‡ç´š] è¨­å®šæ¨¡å‹èˆ‡ API Key ---
# --- ä½¿ç”¨ä¸åŒçš„æ¨¡å‹ä¾†åŸ·è¡Œä¸åŒä»»å‹™ï¼Œæ›´å…·æˆæœ¬æ•ˆç›Š ---
ENHANCER_MODEL = "gemini-2.0-flash" # ç”¨æ–¼å¿«é€Ÿã€ä¾¿å®œçš„å•é¡Œå¼·åŒ–
ANALYSIS_MODEL = "gemini-2.0-flash"   # ç”¨æ–¼è¤‡é›œçš„ç¨‹å¼ç¢¼ç”Ÿæˆèˆ‡æ´å¯Ÿ
API_KEY = os.getenv("GEMINI_API_KEY")

print(f"[llm_core DEBUG] å¼·åŒ–æ¨¡å‹: {ENHANCER_MODEL}")
print(f"[llm_core DEBUG] åˆ†ææ¨¡å‹: {ANALYSIS_MODEL}")

if not API_KEY:
    print("="*50)
    print("è­¦å‘Š [llm_core]: æ‰¾ä¸åˆ° GEMINI_API_KEY (ç’°å¢ƒè®Šæ•¸)ã€‚")
    print("="*50)
else:
    print(f"[llm_core DEBUG] æˆåŠŸè¼‰å…¥ API Key (å‰ 4 ç¢¼): {API_KEY[:4]}...")
    try:
        genai.configure(api_key=API_KEY)
        print("[llm_core DEBUG] Google AI SDK è¨­å®šæˆåŠŸã€‚")
    except Exception as e:
        print(f"[llm_core DEBUG] Google AI SDK è¨­å®šå¤±æ•—: {e}")

# --- 3. è‡ªå‹•æœå°‹ä¸­æ–‡å­—å‹ (ä¿æŒä¸è®Š) ---
def get_chinese_font():
    """åœ¨ç³»çµ±ä¸­è‡ªå‹•æœå°‹å¯ç”¨çš„ä¸­æ–‡å­—å‹"""
    print("[llm_core DEBUG] æ­£åœ¨æœå°‹å¯ç”¨çš„ä¸­æ–‡å­—å‹...")
    font_paths = fm.findSystemFonts(fontpaths=None, fontext='ttf')
    font_name_to_path = {}
    for font_path in font_paths:
        try:
            font_name = fm.FontProperties(fname=font_path).get_name()
            font_name_to_path[font_name] = font_path
        except Exception:
            continue

    preferred_font_names = [
        'Microsoft JhengHei', 'PingFang TC', 'Noto Sans CJK TC', 
        'SimHei', 'Arial Unicode MS',
    ]
    
    for font_name in preferred_font_names:
        if font_name in font_name_to_path:
            print(f"[llm_core DEBUG] æ‰¾åˆ°åå¥½çš„å­—å‹: {font_name}")
            return font_name 

    print("[llm_core DEBUG] æœªæ‰¾åˆ°åå¥½å­—å‹ï¼Œé–‹å§‹æƒæç³»çµ±å­—å‹...")
    for font_path in font_paths:
        try:
            font_prop = fm.FontProperties(fname=font_path)
            if fm.get_font(font_prop).get_glyph_name('ä½ '): 
                print(f"[llm_core DEBUG] æ‰¾åˆ°ä¸€å€‹å¯ç”¨çš„ä¸­æ–‡å­—å‹: {font_path}")
                return font_path 
        except Exception:
            continue
            
    print("[llm_core DEBUG] è­¦å‘Š: ç³»çµ±ä¸­æ‰¾ä¸åˆ°ä»»ä½•å¯ç”¨çš„ä¸­æ–‡å­—å‹ã€‚åœ–è¡¨ä¸­æ–‡å°‡é¡¯ç¤ºç‚ºæ–¹å¡Šã€‚")
    return None

# --- 4. åœ¨ç¨‹å¼å•Ÿå‹•æ™‚ï¼Œå°±å…ˆæ‰¾åˆ°å­—å‹ä¸¦å­˜èµ·ä¾† (ä¿æŒä¸è®Š) ---
GLOBAL_CHINESE_FONT_PATH_OR_NAME = get_chinese_font()


# --- 5. [æ–°å¢] ç§»æ¤è‡ª Streamlit çš„ã€Œæç¤ºè©å¼·åŒ–ã€é‚è¼¯ ---
def enhance_user_prompt(original_prompt: str, schema_info: str) -> str:
    """
    ä½¿ç”¨ LLM å°‡æ¨¡ç³Šçš„ä½¿ç”¨è€…å•é¡Œè½‰åŒ–ç‚ºæ¸…æ™°çš„åˆ†æä»»å‹™ã€‚
    """
    print(f"[llm_core DEBUG] æ­£åœ¨å¼·åŒ–æç¤ºè©: {original_prompt}")
    
    enhancement_system_prompt = f"""
    ä½ æ˜¯ä¸€å€‹è¼”åŠ©ç³»çµ±ï¼Œä½ çš„ä»»å‹™æ˜¯å°‡ä½¿ç”¨è€…çš„ç°¡çŸ­æ•¸æ“šåˆ†æå•é¡Œï¼Œè½‰åŒ–ç‚ºä¸€å€‹æ›´æ¸…æ™°ã€æ›´å®Œæ•´ã€æ›´å…·é«”çš„æ•¸æ“šåˆ†æä»»å‹™æè¿°ï¼Œå¿…é ˆè€ƒæ…®ä½¿ç”¨è€…æ‰€æœ‰æ–¹é¢çš„å¯èƒ½ï¼ŒåŠæ•¸æ“šä¸­æ‰€æœ‰æ¬„ä½çš„é—œè¯æ€§ã€‚
    é€™å€‹æè¿°å°‡è¢«äº¤çµ¦å¦ä¸€å€‹ AI (Python ç¨‹å¼ç¢¼ç”Ÿæˆå™¨) ä¾†åŸ·è¡Œã€‚
    
    ä½ å¿…é ˆè€ƒæ…®ä»¥ä¸‹çš„è³‡æ–™åº« schemaï¼š
    {schema_info}
    
    ä½ çš„è¼¸å‡º**åªèƒ½**åŒ…å«è½‰åŒ–å¾Œçš„ç¹é«”ä¸­æ–‡å•é¡Œæ•˜è¿°ï¼Œä¸è¦æœ‰ä»»ä½•å‰è¨€ã€å¾Œèªæˆ–è§£é‡‹ã€‚

    ç¯„ä¾‹ 1:
    ä½¿ç”¨è€…è¼¸å…¥ï¼šèª°æ˜¯å¤±èª¤ç‹ï¼Ÿ
    ä½ è¼¸å‡ºï¼šè«‹çµ±è¨ˆ 'player' æ¬„ä½ä¸­ 'type' ç‚º 'error' (å¤±èª¤) çš„æ¬¡æ•¸ï¼Œä¸¦æ‰¾å‡ºèª°çš„å¤±èª¤æ¬¡æ•¸æœ€é«˜ï¼Œå°‡çµæœå„²å­˜åœ¨ä¸€å€‹è®Šæ•¸ (ä¾‹å¦‚ 'error_king_name' å’Œ 'error_king_count') ä¸­ã€‚
    
    ç¯„ä¾‹ 2:
    ä½¿ç”¨è€…è¼¸å…¥ï¼šçƒå“¡ A çš„åœ“é¤…åœ–
    ä½ è¼¸å‡ºï¼šè«‹åˆ†æ 'player' æ¬„ä½ç‚º 'A' çš„æ‰€æœ‰æ“Šçƒï¼Œä¸¦ä½¿ç”¨åœ“é¤…åœ–é¡¯ç¤º 'type' (çƒç¨®) çš„åˆ†ä½ˆæ¯”ä¾‹ã€‚
    """
    
    try:
        model = genai.GenerativeModel(ENHANCER_MODEL)
        # --- [é—œéµ] ä½¿ç”¨ä½æº« (temperature=0.2) ç¢ºä¿è½‰è­¯çš„æº–ç¢ºæ€§èˆ‡ä¸€è‡´æ€§ ---
        response = model.generate_content(
            [
                {'role': 'user', 'parts': [enhancement_system_prompt]},
                {'role': 'model', 'parts': ["å¥½çš„ï¼Œæˆ‘æœƒå°‡ä½¿ç”¨è€…çš„å•é¡Œè½‰åŒ–ç‚ºæ¸…æ™°çš„ä»»å‹™ã€‚è«‹çµ¦æˆ‘ä½¿ç”¨è€…çš„å•é¡Œã€‚"]},
                {'role': 'user', 'parts': [original_prompt]}
            ],
            generation_config={"temperature": 0.2} 
        )
        enhanced_prompt = response.text.strip()
        print(f"[llm_core DEBUG] å¼·åŒ–å¾Œçš„æç¤ºè©: {enhanced_prompt}")
        return enhanced_prompt
    except Exception as e:
        print(f"[llm_core DEBUG] æç¤ºè©å¼·åŒ–å¤±æ•—: {e}ã€‚å°‡ä½¿ç”¨åŸå§‹æç¤ºè©ã€‚")
        return original_prompt

# --- 6. [æ–°å¢] ç§»æ¤è‡ª Streamlit çš„ã€Œçµæœæ ¼å¼åŒ–ã€é‚è¼¯ ---
def _format_summary_info_for_prompt(summary_info: dict) -> str:
    """
    å°‡ Python åŸ·è¡Œçµæœçš„æ‘˜è¦å­—å…¸ï¼Œæ ¼å¼åŒ–ç‚ºçµ¦ LLM é–±è®€çš„ Markdown å­—ä¸²ã€‚
    """
    if not summary_info:
        return "AI ç¨‹å¼ç¢¼æœªç”¢ç”Ÿä»»ä½•å¯ä¾›åˆ†æçš„æ‘˜è¦è®Šæ•¸ã€‚"
        
    analysis_context_str = "ç¨‹å¼ç¢¼åŸ·è¡Œå¾Œï¼Œæ“·å–å‡ºä»¥ä¸‹æ ¸å¿ƒè®Šæ•¸èˆ‡å…¶å€¼ï¼š\n\n"
    for name, val in summary_info.items():
        analysis_context_str += f"### è®Šæ•¸ `{name}` (å‹åˆ¥: `{type(val).__name__}`)\n"
        
        if isinstance(val, (pd.DataFrame, pd.Series)):
            try:
                # å˜—è©¦è½‰æ›ç‚º Markdownï¼Œå¦‚æœå¤ªå¤§æˆ–å¤±æ•—å‰‡ç”¨ str
                md = val.to_markdown()
                if len(md) > 1000: # é™åˆ¶é•·åº¦
                    analysis_context_str += f"```\n{str(val)}\n(è³‡æ–™éé•·ï¼Œåƒ…é¡¯ç¤ºéƒ¨åˆ†)\n```\n\n"
                else:
                    analysis_context_str += f"```markdown\n{md}\n```\n\n"
            except Exception:
                analysis_context_str += f"```\n{str(val)}\n```\n\n"
        else:
            analysis_context_str += f"```\n{str(val)}\n```\n\n"
    return analysis_context_str


# --- 7. [é‡å¤§å‡ç´š] æ ¸å¿ƒåˆ†æå‡½æ•¸ ---
def run_analysis(natural_language_prompt: str, history: list = None, max_retries: int = 2) -> dict:
    """
    ã€é‡å¤§å‡ç´šç‰ˆã€‘
    - æ”¯æ´äº¤è«‡è¨˜æ†¶ (history)
    - æ”¯æ´æç¤ºè©å¼·åŒ– (enhance_user_prompt)
    - æ”¯æ´ç¨‹å¼ç¢¼è‡ªæˆ‘ä¿®æ­£ (self-correction loop)
    - æ”¯æ´æ›´å¼·å¤§çš„çµæœæ“·å– (summary_info)
    - ç­–ç•¥æ€§ä½¿ç”¨ temperature
    """
    
    if df is None:
        return {"text": None, "figure": None, "error": "è³‡æ–™é›† 'all_dataset.csv' æœªè¼‰å…¥ã€‚"}
    if not API_KEY:
        return {"text": None, "figure": None, "error": "æœªè¨­å®š GEMINI_API_KEYã€‚"}

    if history is None:
        history = []

    try:
        # --- æ­¥é©Ÿ 0: åˆå§‹åŒ–åˆ†ææ¨¡å‹ ---
        analysis_model = genai.GenerativeModel(ANALYSIS_MODEL)
        
        # --- æ­¥é©Ÿ 1: ã€æ–°ã€‘å¼·åŒ–æç¤ºè© ---
        # (æ­¤æ­¥é©Ÿä½¿ç”¨ ENHANCER_MODELï¼Œå·²åœ¨å‡½æ•¸å…§)
        enhanced_prompt = enhance_user_prompt(natural_language_prompt, data_schema_info)

        # --- æ­¥é©Ÿ 2: ã€ä¿®æ”¹ã€‘ç”Ÿæˆç¨‹å¼ç¢¼ (åŠ å…¥è¨˜æ†¶èˆ‡å­—å‹) ---
        print(f"[llm_core DEBUG] æ­£åœ¨ä½¿ç”¨ {ANALYSIS_MODEL} å‘¼å« Google API (ç”Ÿæˆç¨‹å¼ç¢¼)...")
        
        system_prompt = create_system_prompt(data_schema_info, column_definitions_info)
        
        # --- â–¼ æ³¨å…¥å­—é«”æŒ‡ä»¤ (ä¿æŒä¸è®Š) â–¼ ---
        font_prompt_injection = ""
        if GLOBAL_CHINESE_FONT_PATH_OR_NAME:
            font_path_or_name_str = repr(GLOBAL_CHINESE_FONT_PATH_OR_NAME)
            font_prompt_injection = f"""
            *** EXTREMELY IMPORTANT (FONT SETTING) ***
            You MUST add the following 3 lines of code right after `import matplotlib.pyplot as plt` to set the Chinese font:
            ```python
            import matplotlib.pyplot as plt
            import matplotlib.font_manager as fm
            
            # --- START FONT SETTING ---
            font_path_or_name = {font_path_or_name_str}
            try:
                font_prop = fm.FontProperties(fname=font_path_or_name)
                plt.rcParams['font.sans-serif'] = [font_prop.get_name()]
            except Exception:
                plt.rcParams['font.sans-serif'] = [font_path_or_name]
            plt.rcParams['axes.unicode_minus'] = False # Fix for minus sign
            # --- END FONT SETTING ---
            ```
            ******************************************
            """
        system_prompt += font_prompt_injection
        # --- â–² ä¿®æ”¹å®Œç•¢ â–² ---
        
        # --- ã€ä¿®æ”¹ã€‘çµ„åˆè¨Šæ¯ (åŠ å…¥ history) ---
        messages_for_api = [
            {'role': 'user', 'parts': [system_prompt]},
            {'role': 'model', 'parts': ["å¥½çš„ï¼Œæˆ‘æº–å‚™å¥½äº†ã€‚æˆ‘æœƒä¾ç…§æŒ‡ç¤ºï¼Œåœ¨ `matplotlib` ç¨‹å¼ç¢¼ä¸­åŠ å…¥è¨­å®šä¸­æ–‡å­—å‹çš„å€å¡Šã€‚è«‹çµ¦æˆ‘ä½¿ç”¨è€…çš„å•é¡Œã€‚"]},
        ]
        
        # åŠ å…¥æ­·å²å°è©±
        messages_for_api.extend(history)
        
        # åŠ å…¥æœ¬æ¬¡å¼·åŒ–å¾Œçš„å•é¡Œ
        messages_for_api.append({'role': 'user', 'parts': [enhanced_prompt]})
        
        # --- æ­¥é©Ÿ 3: ã€æ–°ã€‘ç¨‹å¼ç¢¼ç”Ÿæˆèˆ‡è‡ªæˆ‘ä¿®æ­£è¿´åœˆ ---
        code_to_execute = None
        ai_response_text = ""
        
        for attempt in range(max_retries):
            if attempt > 0:
                print(f"[llm_core DEBUG] åµæ¸¬åˆ°éŒ¯èª¤ï¼Œæ­£åœ¨é€²è¡Œç¬¬ {attempt + 1} æ¬¡ä¿®æ­£å˜—è©¦...")
            
            # --- [é—œéµ] ä½¿ç”¨ä½æº« (temperature=0.1) ç¢ºä¿ç¨‹å¼ç¢¼çš„ç²¾ç¢ºæ€§ ---
            response = analysis_model.generate_content(
                messages_for_api,
                generation_config={"temperature": 0.1}
            )
            ai_response_text = response.text
            
            # (1) è§£æç¨‹å¼ç¢¼
            if "```python" in ai_response_text:
                code_start = ai_response_text.find("```python") + len("```python\n")
                code_end = ai_response_text.rfind("```")
                code_to_execute = ai_response_text[code_start:code_end].strip()
            else:
                # AI æ²’æœ‰å›å‚³ç¨‹å¼ç¢¼ï¼Œå¯èƒ½åªæ˜¯ç´”æ–‡å­—å›ç­”
                if not code_to_execute:
                    print("[llm_core DEBUG] AI å›æ‡‰ä¸­æœªåµæ¸¬åˆ°ç¨‹å¼ç¢¼ã€‚")
                    # å¦‚æœæ˜¯ç¬¬ä¸€æ¬¡å˜—è©¦å°±æ²’ç¨‹å¼ç¢¼ï¼Œå¯èƒ½
                    return {"text": ai_response_text, "figure": None, "error": None}


            print("--- [llm_core DEBUG] åµæ¸¬åˆ° AI ç”Ÿæˆçš„ç¨‹å¼ç¢¼ (å˜—è©¦ {}): ---".format(attempt + 1))
            print(code_to_execute)
            print("-------------------------------------------------")
            
            # (2) åŸ·è¡Œç¨‹å¼ç¢¼
            final_fig = None
            summary_info = {} # --- [å‡ç´š] ä½¿ç”¨å­—å…¸æ“·å–çµæœ ---
            
            try:
                print("[llm_core DEBUG] æ­£åœ¨åŸ·è¡Œ AI ç¨‹å¼ç¢¼ (exec)...")
                exec_globals = {
                    "pd": pd, "df": df.copy(),
                    "platform": platform, "io": io
                }
                exec(code_to_execute, exec_globals)
                print("[llm_core DEBUG] ç¨‹å¼ç¢¼åŸ·è¡Œå®Œç•¢ã€‚")
                
                # --- [å‡ç´š] ç§»æ¤è‡ª Streamlit çš„ã€Œçµæœæ“·å–ã€é‚è¼¯ ---
                ignore_list = ['df', 'pd', 'platform', 'io', 'fig', 'np', 'plt', 'sns', 'fm']
                for name, val in exec_globals.items():
                    if name.startswith('_') or name in ignore_list:
                        continue
                    if isinstance(val, (int, float, str, bool)):
                        summary_info[name] = val
                    elif hasattr(val, '__len__') and not isinstance(val, str) and len(val) < 20:
                        summary_info[name] = val
                
                final_fig = exec_globals.get('fig', None)
                print(f"[llm_core DEBUG] æˆåŠŸï¼æ“·å–åˆ° {len(summary_info)} å€‹è®Šæ•¸ã€‚")
                
                # åŸ·è¡ŒæˆåŠŸï¼Œè·³å‡ºä¿®æ­£è¿´åœˆ
                break 
                
            except Exception as e:
                print(f"[llm_core DEBUG] ç¨‹å¼ç¢¼åŸ·è¡Œå¤±æ•—: {e}")
                traceback.print_exc() # å°å‡ºæ›´è©³ç´°çš„éŒ¯èª¤
                error_message = f"ç¨‹å¼ç¢¼åŸ·è¡Œå¤±æ•—: {type(e).__name__}: {e}"
                
                if attempt == max_retries - 1:
                    # é”åˆ°æœ€å¤§é‡è©¦æ¬¡æ•¸ï¼Œå®£å‘Šå¤±æ•—
                    print("[llm_core DEBUG] é”åˆ°æœ€å¤§é‡è©¦æ¬¡æ•¸ï¼Œå®£å‘Šå¤±æ•—ã€‚")
                    return {"text": None, "figure": None, "error": error_message}
                
                # --- [é—œéµ] å»ºç«‹ä¿®æ­£æç¤º ---
                # å‘Šè¨´ AI éŒ¯åœ¨å“ªï¼Œä¸¦è¦æ±‚ä¿®æ­£
                fix_prompt = f"""
                ä½ ä¹‹å‰ç”Ÿæˆçš„ Python ç¨‹å¼ç¢¼åœ¨åŸ·è¡Œæ™‚ç™¼ç”Ÿäº†ä»¥ä¸‹éŒ¯èª¤ï¼š
                
                éŒ¯èª¤é¡å‹: {type(e).__name__}
                éŒ¯èª¤è¨Šæ¯: {e}

                é€™æ˜¯ä½ ä¹‹å‰ç”Ÿæˆçš„ (éŒ¯èª¤çš„) ç¨‹å¼ç¢¼ï¼š
                ```python
                {code_to_execute}
                ```
                
                è«‹ä¿®æ­£é€™å€‹éŒ¯èª¤ï¼Œä¸¦**åª**æä¾›ä¿®æ­£å¾Œçš„å®Œæ•´ Python ç¨‹å¼ç¢¼å€å¡Š (```python ... ```)ã€‚
                """
                # å°‡ä¿®æ­£è«‹æ±‚åŠ å…¥åˆ°å°è©±æ­·å²ä¸­ï¼Œæº–å‚™ä¸‹ä¸€æ¬¡è¿´åœˆ
                messages_for_api.append({'role': 'model', 'parts': [ai_response_text]}) # AI çš„éŒ¯èª¤å›ç­”
                messages_for_api.append({'role': 'user', 'parts': [fix_prompt]})      # æˆ‘å€‘çš„ä¿®æ­£è«‹æ±‚
        
        # --- æ­¥é©Ÿ 4: ã€å‡ç´šã€‘ç¬¬äºŒæ¬¡ AI å‘¼å« (ç”Ÿæˆæ´å¯Ÿ) ---
        print("[llm_core DEBUG] æ­£åœ¨å‘¼å« Google API (ç”Ÿæˆæ´å¯Ÿ)...")
        
        # (1) æ ¼å¼åŒ– summary_info
        analysis_context_str = _format_summary_info_for_prompt(summary_info)
        
        # (2) å»ºç«‹æ´å¯Ÿæç¤º
        # --- [å‡ç´š] ç§»æ¤è‡ª Streamlit çš„ã€Œæ´å¯Ÿæç¤ºã€é‚è¼¯ ---
        insight_prompt = f"""
        ä½ æ˜¯ä¸€ä½å°ˆæ¥­çš„ç¾½çƒæ•¸æ“šåˆ†æå¸«ã€‚
        ä½¿ç”¨è€…çš„åŸå§‹å•é¡Œæ˜¯ï¼šã€Œ{natural_language_prompt}ã€
        
        æ ¹æ“šé€™å€‹å•é¡Œï¼ŒAI ç”¢ç”Ÿä¸¦åŸ·è¡Œäº†ä¸€æ®µ Python ç¨‹å¼ç¢¼ï¼Œç¨‹å¼ç¢¼åŸ·è¡Œå¾Œç”¢ç”Ÿçš„æ ¸å¿ƒæ•¸æ“šè®Šæ•¸å¦‚ä¸‹ã€‚

        --- æ ¸å¿ƒæ•¸æ“šè®Šæ•¸ ---
        {analysis_context_str}
        --- æ ¸å¿ƒæ•¸æ“šè®Šæ•¸çµæŸ ---

        è«‹ä½ åŸºæ–¼ã€Œä½¿ç”¨è€…å•é¡Œã€å’Œä¸Šè¿°æ‰€æœ‰ã€Œæ ¸å¿ƒæ•¸æ“šè®Šæ•¸ã€ï¼Œç”¨ç¹é«”ä¸­æ–‡æ’°å¯«ä¸€ä»½ç²¾ç°¡ã€æ¢ç†åˆ†æ˜çš„æ•¸æ“šæ´å¯Ÿå ±å‘Šã€‚
        å ±å‘Šæ‡‰åŒ…å«ä»¥ä¸‹éƒ¨åˆ†ï¼š
        1.  **ç›´æ¥å›ç­”**ï¼šç›´æ¥ä¸”æ˜ç¢ºåœ°å›ç­”ä½¿ç”¨è€…çš„å•é¡Œã€‚
        2.  **é—œéµç™¼ç¾**ï¼šå¾æ•¸æ“šä¸­æç…‰å‡º 1 åˆ° 3 å€‹æœ€é—œéµçš„è§€å¯Ÿæˆ–è¶¨å‹¢ã€‚
        3.  **ç¸½çµ**ï¼šç”¨ä¸€å¥è©±ç¸½çµåˆ†æçµæœã€‚
        
        è«‹é¿å…é‡è¤‡æè¿°æ•¸æ“šå…§å®¹ï¼Œå°ˆæ³¨æ–¼æä¾›æœ‰åƒ¹å€¼çš„è¦‹è§£ã€‚
        """

        try:
            # --- [é—œéµ] ä½¿ç”¨ä¸­ä½æº« (temperature=0.4) ç¢ºä¿æ´å¯Ÿçš„å°ˆæ¥­æ€§èˆ‡å¯è®€æ€§ ---
            insight_response = analysis_model.generate_content(
                insight_prompt,
                generation_config={"temperature": 0.4}
            )
            summary_text = insight_response.text
            print("[llm_core DEBUG] AI æ´å¯Ÿç”Ÿæˆå®Œç•¢ã€‚")
        except Exception as e:
            summary_text = f"*(ç„¡æ³•è‡ªå‹•ç”Ÿæˆæ•¸æ“šæ´å¯Ÿ: {e})*"
            print(f"[llm_core DEBUG] AI æ´å¯Ÿç”Ÿæˆå¤±æ•—: {e}")


        # --- æ­¥é©Ÿ 5: ã€ä¿®æ”¹ã€‘çµ„åˆæœ€çµ‚çµæœ (æ”¯æ´æ­·å²) ---
        
        # é€™æ˜¯ AI ç¬¬ä¸€æ¬¡çš„å›æ‡‰ (åŒ…å«ç¨‹å¼ç¢¼)
        code_block_for_history = ai_response_text
        
        # é€™æ˜¯ AI ç¬¬äºŒæ¬¡çš„å›æ‡‰ (æ´å¯Ÿ)
        # æˆ‘å€‘å°‡å…©è€…çµ„åˆèµ·ä¾†ï¼Œå„²å­˜åˆ° history ä¸­
        final_content_for_history = (
            f"{code_block_for_history}\n\n"
            f"---\n"
            f"### ğŸ“Š æ•¸æ“šæ´å¯Ÿ\n"
            f"{summary_text}"
        )
        
        return {
            "text": summary_text,  # æœ€çµ‚çš„æ´å¯Ÿæ–‡å­—
            "figure": final_fig,           # æœ€çµ‚çš„åœ–è¡¨ç‰©ä»¶
            "code_executed": code_to_execute, # æœ€çµ‚ (æˆ–ä¿®æ­£å¾Œ) åŸ·è¡Œçš„ç¨‹å¼ç¢¼
            "error": None,
            
            # --- [é—œéµ] å›å‚³é€™å…©é …ï¼Œç”¨æ–¼å»ºç«‹ä¸‹ä¸€æ¬¡å‘¼å«çš„ history ---
            "history_user": {"role": "user", "parts": [natural_language_prompt]}, # å„²å­˜ã€ŒåŸå§‹ã€å•é¡Œ
            "history_model": {"role": "model", "parts": [final_content_for_history.strip()]}
        }

    except Exception as e:
        print(f"[llm_core DEBUG] run_analysis åŸ·è¡Œæ™‚ç™¼ç”Ÿåš´é‡éŒ¯èª¤: {e}")
        traceback.print_exc()
        return {"text": None, "figure": None, "error": str(e)}


# --- (ä¿æŒä¸è®Š) å„€è¡¨æ¿ç¿»è­¯å™¨ ---
def generate_analysis_from_dashboard(session_id: str, attribute: str, search_query: str) -> dict:
    """
    (ä¿æŒä¸è®Š)
    å°‡å„€è¡¨æ¿çš„ã€Œé¸é …ã€è½‰æ›æˆã€Œè‡ªç„¶èªè¨€å•é¡Œã€ã€‚
    """
    
    prompt = f"è«‹å¹«æˆ‘åˆ†ææ‰€æœ‰å ´æ¬¡çš„æ•¸æ“šã€‚"
    
    if search_query:
        prompt += f" è«‹ç‰¹åˆ¥é‡å°å­¸ç”Ÿ '{search_query}' é€²è¡Œåˆ†æã€‚"
    
    if attribute == "ALL (ç¸½è¦½)":
        prompt += " è«‹æä¾›é€™å€‹å ´æ¬¡çš„æ•´é«”æ•¸æ“šç¸½è¦½ï¼Œä¸¦ç”¨ä¸€å€‹åˆé©çš„åœ–è¡¨ï¼ˆä¾‹å¦‚é•·æ¢åœ–æˆ–åœ“é¤…åœ–ï¼‰ä¾†è¦–è¦ºåŒ–é—œéµæŒ‡æ¨™ã€‚"
    
    elif attribute == "çƒç¨®":
        prompt += f" è«‹åˆ†æé€™å€‹å ´æ¬¡çš„ã€Œçƒç¨®ã€åˆ†ä½ˆã€‚è«‹ä½¿ç”¨åœ“é¤…åœ– (pie chart) æˆ–é•·æ¢åœ– (bar chart) ä¾†å‘ˆç¾ä¸åŒçƒç¨® (ä¾‹å¦‚ï¼šæ®ºçƒ, åˆ‡çƒ, Tiao, é«˜é çƒ) çš„ä½¿ç”¨æ¬¡æ•¸æˆ–ç™¾åˆ†æ¯”ã€‚"
    
    else:
        prompt += f" è«‹å°ˆæ³¨æ–¼åˆ†æ '{attribute}' é€™å€‹æŒ‡æ¨™ï¼Œä¸¦ç‚ºæ­¤ç”Ÿæˆä¸€å€‹æœ€åˆé©çš„åœ–è¡¨ã€‚"
    
    print(f"[llm_core] ç¿»è­¯å¾Œçš„ Prompt: {prompt}")
    
    # --- æ³¨æ„ï¼šé€™è£¡æˆ‘å€‘ã€Œæ²’æœ‰ã€å‚³å…¥ history ---
    # --- é€™è¡¨ç¤ºå¾å„€è¡¨æ¿é»æ“Šçš„åˆ†æï¼Œæ°¸é éƒ½æ˜¯ã€Œæ–°çš„å°è©±ã€---
    return run_analysis(prompt, history=None)


# --- [æ–°å¢] ä¸»ç¨‹å¼é€²å…¥é» (ç”¨æ–¼æ¸¬è©¦) ---
if __name__ == "__main__":
    # é€™æ˜¯ä¸€å€‹ç¯„ä¾‹ï¼Œå±•ç¤ºå¦‚ä½•ä½¿ç”¨ã€Œäº¤è«‡è¨˜æ†¶ã€
    
    print("\n" + "="*80)
    print(" ğŸš€ æ­£åœ¨å•Ÿå‹• LLM Core æ¸¬è©¦ (æ”¯æ´è¨˜æ†¶)...")
    print("="*80 + "\n")

    conversation_history = []
    
    # --- ç¬¬ 1 æ¬¡æå• ---
    print("--- æå• 1: 'èª°æ˜¯å¤±èª¤ç‹ï¼Ÿ' ---")
    question1 = "èª°æ˜¯å¤±èª¤ç‹ï¼Ÿ"
    result1 = run_analysis(question1, history=conversation_history)
    
    if result1["error"]:
        print(f"éŒ¯èª¤: {result1['error']}")
    else:
        print("\n[AI æ´å¯Ÿ 1]:")
        print(result1["text_insight"])
        if result1["figure"]:
            print("(å·²ç”Ÿæˆåœ–è¡¨ 1)")
        
        # å„²å­˜è¨˜æ†¶
        conversation_history.append(result1["history_user"])
        conversation_history.append(result1["history_model"])
        
    print("\n" + "="*80 + "\n")

    # --- ç¬¬ 2 æ¬¡æå• (åˆ©ç”¨è¨˜æ†¶) ---
    print("--- æå• 2: 'é‚£çƒå“¡ A çš„æ®ºçƒ (smash) æ¬¡æ•¸å‘¢ï¼Ÿ' ---")
    question2 = "é‚£çƒå“¡ A çš„æ®ºçƒ (smash) æ¬¡æ•¸å‘¢ï¼Ÿ"
    
    # --- [é—œéµ] å‚³å…¥æ›´æ–°å¾Œçš„ conversation_history ---
    result2 = run_analysis(question2, history=conversation_history)
    
    if result2["error"]:
        print(f"éŒ¯èª¤: {result2['error']}")
    else:
        print("\n[AI æ´å¯Ÿ 2]:")
        print(result2["text_insight"])
        if result2["figure"]:
            print("(å·²ç”Ÿæˆåœ–è¡¨ 2)")
            
        # å†æ¬¡å„²å­˜è¨˜æ†¶
        conversation_history.append(result2["history_user"])
        conversation_history.append(result2["history_model"])

    print("\n" + "="*80 + "\n")
    print("æ¸¬è©¦å®Œç•¢ã€‚")